# 一、什么是文本挖掘？

- 文本挖掘的意义就是从文本数据中寻找有价值的信息，来发现或者解决一些实际问题。文本挖掘中最重要最基本的应用是实现文本的分类和聚类，前者是有监督的挖掘算法，后者是无监督的挖掘算法。

- **wiki版本**：
    文本挖掘，也称为文本数据挖掘，大致相当于文本分析，是从文本中获取高质量信息的过程。文本挖掘通常涉及构造输入文本的过程（通常解析，添加一些派生的语言特征和删除其他特征，然后插入到数据库中），在结构化数据中导出模式，最后评估和解释输出。典型的文本挖掘任务包括文本分类，文本聚类，概念/实体提取，粒度分类法的生成，情感分析，文档摘要和实体关系建模（即，命名实体之间的学习关系）。

# 二、文本挖掘的步骤

- Step1. 文本数据收集
- Step2. 文本预处理
 * Step2.1 文本数据清洗
 * Step2.2 文本分词（自定义字典➡️自定义停止词➡️分词）
- Step3. 构建文档-词条矩阵并转换为数据框
- Step4. 对数据框建立统计、挖掘模型
- Step5. 模型评估

# 三、文本挖掘的分类

## 1、关键词提取
对长文本的内容进行分析，输出能够反映文本关键信息的关键词。

## 2、文本摘要
许多文本挖掘应用程序需要总结文本文档，以便对大型文档或某一主题的文档集合做出简要概述。

## 3、聚类
聚类是从未标注文本中获取隐藏数据结构的技术，常见的有 K均值聚类和层次聚类。

## 4、文本分类
文本分类使用监督学习的方法，以对未知数据的分类进行预测。

## 5、文本主题模型
LDA（Latent Dirichlet Allocation）是一种文档主题生成模型，为一个三层贝叶斯概率模型，包含词、主题和文档三层结构。

## 6、观点抽取
对文本（主要针对评论）进行分析，抽取出核心观点，并判断极性(正负面)，主要用于电商、美食、酒店、汽车等评论进行分析。

## 7、情感分析
对文本进行情感倾向判断，将文本情感分为正向、负向、中性。用于口碑分析、话题监控、舆情分析。


# 四、中文文本挖掘预处理总结

## 1、收集数据
## 2、除去数据中非文本部分
少量的非文本内容可以用 python 的正则表达式（re）删除，复杂的则可以用 beautifulsoup 来去除。

## 3、中文分词
常见的中文分词软件有很多，比如 jieba 分词。

## 4、引用停用词
在分词后的文本中有很多无效的词，比如“着”，“和”，还有一些标点符号，这些我们不想在文本分析的时候引入，因此需要去掉，这些词就是停用词。常用的中文停用词表是1208个，下载地址在这。

在我们用scikit-learn做特征处理的时候，可以通过参数stop_words来引入一个数组作为停用词表。

## 5、特征处理
现在我们就可以用scikit-learn来对我们的文本特征进行处理了，在文本挖掘预处理之向量化与Hash Trick中，我们讲到了两种特征处理的方法，向量化与Hash Trick。而向量化是最常用的方法，因为它可以接着进行TF-IDF的特征处理。在文本挖掘预处理之TF-IDF中，我们也讲到了TF-IDF特征处理的方法。

### （1）词袋模型
词袋模型（Bag of Words, BoW）：词袋模型假设我们不考虑文本中词与词之间的上下文关系，仅仅只考虑所有词的权重。而权重与词在文本中出现的频率有关。

词袋模型三部曲：分词（tokenizing），统计修订词特征值（counting）与标准化（normalizing）

词袋模型局限性：它仅仅考虑了词频，没有考虑上下文的关系，因此会丢失一部分文本的语义。但是大多数时候，如果我们的目的是分类聚类，则词袋模型表现的很好。

### （2）特征处理之向量化
在词袋模型的统计词频这一步，我们会得到该文本中所有词的词频，有了词频，我们就可以用词向量表示这个文本。

Scikit-learn 的 CountVectorizer 类可以帮我们完成文本的词频统计与向量化。

向量化的方法很好用，也很直接，但是在有些场景下很难使用，比如分词后的词汇表非常大，达到100万+，此时如果我们直接使用向量化的方法，将对应的样本对应特征矩阵载入内存，有可能将内存撑爆，在这种情况下我们怎么办呢？第一反应是我们要进行特征的降维，说的没错！而Hash Trick就是非常常用的文本特征降维方法。

### （3）特征处理之 Hash Trick
#### A. 什么是 Hash trick？
Hashing trick，有时候也叫做feature hashing，是自然语言处理中降维的手段。在一般的机器学习任务中，它也可以对categorical feature进行降维。

举个例子，比如你是淘宝的算法工程师，你要做一个退货的预测模型，假设有一个 feature 是 location_id，表示商品的产地。这个是 categorical feature ，所以你通常需要做 one-hot encoding，把这一列转化为 dummy variable。商品来自全国各市、全球各国，可能这个 location_id 就有成千上万个数值。转码之后，模型就会增加这一万个 dummy 变量。这对数据的读取、操作，模型的训练都是极大的挑战。

Hashing trick 就是用 hashing function 这个小技巧来降维。若 location_id 都是整数，我们可以对所有的 location_id 取余，location_id (mod p)，这个取余函数就是我们使用的 hashing function。很显然进行取余操作之后，我们最多只有p个不同的数值了。在此之上再用 one-hot encoding，我们只增加了p列。

#### B. Hash trick 的优缺点
Hashing trick有三个主要的优点

1. 降维程度大

2. 计算快速、方便

3. 不要额外的存储空间（额外的参考词典等）

但是，也有些缺点。比如我们观察到上面产地编号9126和21除以5的余数都是1，它们就被放到了一起。在Hashing trick中，这种冲突和合并是无法避免的。但是根据一些论文和大量业界应用的结果，这种冲突合并对预测模型的表现的影响微乎其微。另一个缺点，因为大量的数值并合并，这使得模型和结果不易interpret。

### （4）文本挖掘预处理之TF-IDF
TF-IDF是Term Frequency -  Inverse Document Frequency的缩写，即“词频-逆文本频率”。它由两部分组成，TF和IDF。

前面的TF也就是我们前面说到的词频，我们之前做的向量化也就是做了文本中各个词的出现频率统计，并作为文本特征，这个很好理解。关键是后面的这个IDF，即“逆文本频率”如何理解。在上一节中，我们讲到几乎所有文本都会出现的"to"其词频虽然高，但是重要性却应该比词频低的"China"和“Travel”要低。我们的IDF就是来帮助我们来反应这个词的重要性的，进而修正仅仅用词频表示的词特征值。

概括来讲， IDF 反应了一个词在所有文本中出现的频率，如果一个词在很多的文本中出现，那么它的IDF值应该低，比如上文中的“to”。而反过来如果一个词在比较少的文本中出现，那么它的IDF值应该高。比如一些专业的名词如“Machine Learning”。这样的词IDF值应该高。一个极端的情况，如果一个词在所有的文本中都出现，那么它的IDF值应该为0。

一个词 𝑥 的 IDF 的基本公式如下：
$$IDF(x)=\log \frac{N(x)+1}{N+1}+1$$
其中，$𝑁$代表语料库中文本的总数，而$𝑁(𝑥)$代表语料库中包含词𝑥的文本总数。

有了 IDF 的定义，我们就可以计算某一个词的 TF-IDF 值了：

$$TF−IDF(x)=TF(x)∗IDF(x)$$

其中 TF(x) 指词 𝑥 在当前文本中的词频。

TF-IDF 是非常常用的文本挖掘预处理基本步骤，但是如果预处理中使用了Hash Trick，则一般就无法使用TF-IDF了，因为Hash Trick后我们已经无法得到哈希后的各特征的IDF的值。使用了IF-IDF并标准化以后，我们就可以使用各个文本的词特征向量作为文本的特征，进行分类或者聚类分析。

## 6、建立分析模型
有了每段文本的TF-IDF的特征向量，我们就可以利用这些数据建立分类模型，或者聚类模型了，或者进行主题模型的分析。

